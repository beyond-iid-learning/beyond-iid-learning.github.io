<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Beyond i.i.d. learning: Causality, dynamics, and interactions on</title><link>https://beyond-iid-learning.xyz/</link><description>Recent content in Beyond i.i.d. learning: Causality, dynamics, and interactions on</description><generator>Hugo -- gohugo.io</generator><language>en</language><atom:link href="https://beyond-iid-learning.xyz/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://beyond-iid-learning.xyz/exam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/exam/</guid><description>Exam On this site, you find information about the final examination. Please check regularly for updates.
General information Date: December 22, 2021 Format: multiple choice Grade: pass/fail As of now, we only plan to offer the exam in person (i.e., no online exam) Practice problems To help you prepare for the exam and get a feeling for the type of questions that we will be asking, we plan to provide a test of practice problems for each lecture.</description></item><item><title>A brief introduction to online learning and bandits</title><link>https://beyond-iid-learning.xyz/lectures/lecture-04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-04/</guid><description>This lecture will provide an introduction to (non-statistical) online learning and multi-armed bandits. We will discuss the multiplicative weights algorithm Hedge, and its partial information counterpart EXP3, as well as some applications to learning in games.</description></item><item><title>A brief overview of dynamics and control</title><link>https://beyond-iid-learning.xyz/lectures/lecture-05/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-05/</guid><description>The lecture will summarize the basics of dynamical systems and control theory. We will discuss discrete-time and continuous-time dynamical systems, introduce the concept of equilibria and Lyapunov stability. An important aspect of the lecture will be to emphasize the difference between noise and structural (epistemic) uncertainty and show how uncertainty can be reduced with feedback. We will also discuss connections to game theory and generalization (Lecture 1).</description></item><item><title>A brief overview of game theory</title><link>https://beyond-iid-learning.xyz/lectures/lecture-02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-02/</guid><description>The lecture will summarize key ideas in game theory. Game theory provides a means for modelling interactions between machine learning algorithms and their environment. We will revisit zero-sum games and von Neumann’s minimax theorem and introduce the concept of Nash equilibria. We will then discuss repeated games and adaptive decision-making algorithms (follow the leader, follow a random leader, multiplicative weights).</description></item><item><title>A brief overview of statistical learning theory</title><link>https://beyond-iid-learning.xyz/lectures/lecture-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-01/</guid><description>The lecture will summarize the main ideas of statistical learning theory. We will revisit the standard generalization bounds that characterize the difference between true and empirical risk. We will critically discuss the underlying assumptions and show examples where these are violated. We will also discuss the dependence of the bounds on the number of parameters, which is important for understanding the success of overparametrization in today’s machine learning practice.</description></item><item><title>Announcements</title><link>https://beyond-iid-learning.xyz/announcements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/announcements/</guid><description>Announcements Nov 17 We updated the practice problems for the first two lectures to include short explanations of the correct answers. Nov 11 The lecture slides and the recording for lecture 7 are now available online. Nov 4 The recording for lecture 6 is now available online. Nov 3 We have added a new page with information about the final exam, including a set of practice problems.</description></item><item><title>Deep learning for the discovery of parsimonious physics models</title><link>https://beyond-iid-learning.xyz/lectures/lecture-08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-08/</guid><description>A major challenge in the study of dynamical systems is that of model discovery: turning data into reduced order models that are not just predictive, but provide insight into the nature of the underlying dynamical system that generated the data. We introduce a number of data-driven strategies for discovering nonlinear multiscale dynamical systems and their embeddings from data. We consider two canonical cases: (i) systems for which we have full measurements of the governing variables, and (ii) systems for which we have incomplete measurements.</description></item><item><title>Developing Counterfactual Explanations</title><link>https://beyond-iid-learning.xyz/lectures/lecture-06/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-06/</guid><description>Counterfactual Explanations are a relatively recent form of explanation designed to explicitly meet the needs of non-technical users. Unlike methods such as Shapley values that providing measures of the relative importance of features, counterfactual explanations offer simple direct explanations of the form: You were not offered a loan because your salary was $30k, if it had been $45k instead, you would have been offered the loan. These forms of explanation are very popular in legal and governance areas of AI, and are cited in the guidelines to the GDPR.</description></item><item><title>Equilibrium Computation and Machine Learning</title><link>https://beyond-iid-learning.xyz/lectures/lecture-07/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-07/</guid><description>Machine learning has recently made significant advances in single-agent learning challenges, much of that progress being fueled by the empirical success of gradient descent-based methods in computing local optima of non-convex optimization problems. In multi-agent learning challenges, the role of single-objective optimization is played by equilibrium computation. On this front, however, optimization methods have remained less successful in settings, such as adversarial training and multi-agent reinforcement learning, motivated by deep learning applications.</description></item><item><title>Frequently Asked Questions</title><link>https://beyond-iid-learning.xyz/faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/faq/</guid><description>Frequently Asked Questions In this section, we will collect frequently asked questions as well as the answers. Please check the FAQ before contacting the lecturers or TAs about a question.
I have a question — who should I contact? If your question concerns the exam, please get in touch with Maximilian Mordig. In case you have noticed an issue with the website (e.g., something is broken or not up-to-date), email Timothy Gebhard or create an issue on GitHub.</description></item><item><title>Introduction</title><link>https://beyond-iid-learning.xyz/lectures/lecture-00/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-00/</guid><description>Brief meeting, discussion of course schedule, exam.</description></item><item><title>Introduction to Causality</title><link>https://beyond-iid-learning.xyz/lectures/lecture-03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-03/</guid><description>The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given.</description></item><item><title>Syllabus</title><link>https://beyond-iid-learning.xyz/syllabus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/syllabus/</guid><description>Syllabus Lecturers: Michael Muehlebach, Bernhard Schölkopf, Andreas Krause
Course code: 263-5156-00L
Abstract: Many machine learning problems go beyond supervised learning on independent data points and require an understanding of the underlying causal mechanisms, the interactions between the learning algorithms and their environment, and adaptation to temporal changes. The course highlights some of these challenges and relates them to state-of-the-art research.
Objective: The goal of this seminar is to gain experience with machine learning research and foster interdisciplinary thinking.</description></item><item><title>Symbolic regression and equation learning for identifying sparse non‑linear models</title><link>https://beyond-iid-learning.xyz/lectures/lecture-09/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-09/</guid><description>In classical machine learning, regression is treated as a black box process of identifying a suitable function without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. Following the theme of the lecture by Nathan Kutz, we will consider the search for parsimonious models. In this lecture we will consider non-linear models represented by concise analytical expressions.</description></item><item><title>TBA</title><link>https://beyond-iid-learning.xyz/lectures/lecture-10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-10/</guid><description>TBA</description></item><item><title>TBA</title><link>https://beyond-iid-learning.xyz/lectures/lecture-11/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-11/</guid><description>TBA</description></item><item><title>TBA</title><link>https://beyond-iid-learning.xyz/lectures/lecture-12/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://beyond-iid-learning.xyz/lectures/lecture-12/</guid><description>TBA</description></item></channel></rss>