<!doctype html><html><head>
<meta charset=utf-8>
<meta name=HandheldFriendly content="True">
<meta name=MobileOptimized content="320">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=referrer content="no-referrer">
<meta name=description content>
<title>
A brief overview of statistical learning theory
</title>
<link rel=canonical href=https://beyond-iid-learning.github.io/lectures/lecture-01/>
<link rel=stylesheet href=https://beyond-iid-learning.github.io/styles.css>
<link rel=apple-touch-icon sizes=180x180 href=https://beyond-iid-learning.github.io/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=https://beyond-iid-learning.github.io/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=https://beyond-iid-learning.github.io/favicon-16x16.png>
<link rel=manifest href=https://beyond-iid-learning.github.io/site.webmanifest>
</head><body><section id=header>
<h1><a href=https://beyond-iid-learning.github.io/>Beyond i.i.d. learning:<br> Causality, dynamics, and interactions</a></h1>
<div id=navigation>
<ul>
<li><strong><a href=https://beyond-iid-learning.github.io/syllabus>Syllabus</a></strong></li>
<li><strong><a href=https://beyond-iid-learning.github.io/lectures>Lectures</a></strong></li>
<li><strong><a href=https://beyond-iid-learning.github.io/faq>FAQ</a></strong></li>
</ul>
</div>
</section><div id=content>
<section id=content>
<h1>
<a href=https://beyond-iid-learning.github.io/lectures/lecture-01/>Lecture 1: A brief overview of statistical learning theory</a>
</h1>
<p>
<h4 class=inline>Lecturer:</h4>
<a href=https://sites.google.com/view/mmuehlebach/>Michael Muehlebach</a> (MPI-IS)
<br>
<h4 class=inline>Date:</h4>
<time datetime=2021-09-29>September 29, 2021</time>
<br>
<h4 class=inline>Time:</h4>
<time datetime=16:15>16:15</time>–<time datetime=18:00>18:00</time>
</p>
<p>
<h4>Abstract:</h4>
The lecture will summarize the main ideas of statistical learning theory.
We will revisit the standard generalization bounds that characterize the difference between true and empirical risk.
We will critically discuss the underlying assumptions and show examples where these are violated.
We will also discuss the dependence of the bounds on the number of parameters, which is important for understanding the success of overparametrization in today’s machine learning practice.
</p>
<p class=references>
<h4>References:</h4>
<ul>
<li>von Luxburg, U., & Schölkopf, B. (2011). <em>Statistical Learning Theory: Models, Concepts, and Results.</em> In: <em>Handbook of the History of Logic, Volume 10: Inductive Logic</em> (pp. 651–706). Amsterdam: Elsevier. DOI: <a href=https://doi.org/10.1016/b978-0-444-52936-7.50016-1>10.1016/b978-0-444-52936-7.50016-1</a>.</li>
</ul>
</p>
</section>
</div>
<section id=footer>
Made with <a href=https://gohugo.io/>Hugo</a>.
</section>
</body>
</html>
</body>
</html>