BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Beyond i.i.d. learning//EN
CALSCALE:GREGORIAN
METHOD:PUBLISH
X-WR-CALNAME:Beyond i.i.d. learning
X-WR-TIMEZONE:Europe/Berlin




BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 4 (A brief introduction to online learning and bandits)
DESCRIPTION:Title: A brief introduction to online learning and bandits\nLecturer: Andreas Krause\n\nAbstract:\nThis lecture will provide an introduction to (non-statistical) online learning and multi-armed bandits. We will discuss the multiplicative weights algorithm Hedge, and its partial information counterpart EXP3, as well as some applications to learning in games.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-04/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211020T161500
DTEND:20211020T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 5 (A brief overview of dynamics and control)
DESCRIPTION:Title: A brief overview of dynamics and control\nLecturer: Michael Muehlebach\n\nAbstract:\nThe lecture will summarize the basics of dynamical systems and control theory. We will discuss discrete-time and continuous-time dynamical systems, introduce the concept of equilibria and Lyapunov stability. An important aspect of the lecture will be to emphasize the difference between noise and structural (epistemic) uncertainty and show how uncertainty can be reduced with feedback. We will also discuss connections to game theory and generalization (Lecture 1).
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-05/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211027T161500
DTEND:20211027T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 2 (A brief overview of game theory)
DESCRIPTION:Title: A brief overview of game theory\nLecturer: Michael Muehlebach\n\nAbstract:\nThe lecture will summarize key ideas in game theory. Game theory provides a means for modelling interactions between machine learning algorithms and their environment. We will revisit zero-sum games and von Neumann’s minimax theorem and introduce the concept of Nash equilibria. We will then discuss repeated games and adaptive decision-making algorithms (follow the leader, follow a random leader, multiplicative weights).
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-02/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211006T161500
DTEND:20211006T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 1 (A brief overview of statistical learning theory)
DESCRIPTION:Title: A brief overview of statistical learning theory\nLecturer: Michael Muehlebach\n\nAbstract:\nThe lecture will summarize the main ideas of statistical learning theory. We will revisit the standard generalization bounds that characterize the difference between true and empirical risk. We will critically discuss the underlying assumptions and show examples where these are violated. We will also discuss the dependence of the bounds on the number of parameters, which is important for understanding the success of overparametrization in today’s machine learning practice.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-01/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20210929T161500
DTEND:20210929T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 8 (Deep learning for the discovery of parsimonious physics models)
DESCRIPTION:Title: Deep learning for the discovery of parsimonious physics models\nLecturer: Nathan Kutz\n\nAbstract:\nA major challenge in the study of dynamical systems is that of model discovery: turning data into reduced order models that are not just predictive, but provide insight into the nature of the underlying dynamical system that generated the data. We introduce a number of data-driven strategies for discovering nonlinear multiscale dynamical systems and their embeddings from data. We consider two canonical cases: (i) systems for which we have full measurements of the governing variables, and (ii) systems for which we have incomplete measurements. For systems with full state measurements, we show that the recent sparse identification of nonlinear dynamical systems (SINDy) method can discover governing equations with relatively little data and introduce a sampling method that allows SINDy to scale efficiently to problems with multiple time scales, noise and parametric dependencies. For systems with incomplete observations, we show that the Hankel alternative view of Koopman (HAVOK) method, based on time-delay embedding coordinates and the dynamic mode decomposition, can be used to obtain a linear models and Koopman invariant measurement systems that nearly perfectly captures the dynamics of nonlinear quasiperiodic systems. Neural networks are used in targeted ways to aid in the model reduction process. Together, these approaches provide a suite of mathematical strategies for reducing the data required to discover and model nonlinear multiscale systems.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-08/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211117T161500
DTEND:20211117T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 6 (Developing Counterfactual Explanations)
DESCRIPTION:Title: Developing Counterfactual Explanations\nLecturer: Chris Russell\n\nAbstract:\nCounterfactual Explanations are a relatively recent form of explanation designed to explicitly meet the needs of non-technical users. Unlike methods such as Shapley values that providing measures of the relative importance of features, counterfactual explanations offer simple direct explanations of the form: You were not offered a loan because your salary was $30k, if it had been $45k instead, you would have been offered the loan. These forms of explanation are very popular in legal and governance areas of AI, and are cited in the guidelines to the GDPR. We will discuss the development of counterfactuals explanations, including why previous forms of explanation are often unsuitable for end-users, and the limitations of counterfactual explanations, and what they can and can not tell you, and the challenges of extending them to high-dimensional problems in computer vision.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-06/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211103T161500
DTEND:20211103T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 7 (Equilibrium Computation and Machine Learning)
DESCRIPTION:Title: Equilibrium Computation and Machine Learning\nLecturer: Constantinos Daskalakis\n\nAbstract:\nMachine learning has recently made significant advances in single-agent learning challenges, much of that progress being fueled by the empirical success of gradient descent-based methods in computing local optima of non-convex optimization problems. In multi-agent learning challenges, the role of single-objective optimization is played by equilibrium computation. On this front, however, optimization methods have remained less successful in settings, such as adversarial training and multi-agent reinforcement learning, motivated by deep learning applications. Gradient-descent based methods commonly fail to identify equilibria, and even computing local approximate equilibria has remained daunting. We discuss equilibrium computation challenges motivated by machine learning applications through a combination of learning-theoretic, complexity-theoretic, game-theoretic and topological techniques, presenting obstacles and opportunities for machine learning and game theory going forward. No deep learning / complexity theory knowledge will be assumed for this talk.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-07/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211110T161500
DTEND:20211110T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 0 (Introduction)
DESCRIPTION:Title: Introduction\nLecturer: Bernhard Schölkopf, Michael Muehlebach\n\nAbstract:\nBrief meeting, discussion of course schedule, exam.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-00/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20210922T173000
DTEND:20210922T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 3 (Introduction to Causality)
DESCRIPTION:Title: Introduction to Causality\nLecturer: Bernhard Schölkopf\n\nAbstract:\nThe two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-03/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211013T161500
DTEND:20211013T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 9 (Symbolic regression and equation learning for identifying sparse non‑linear models)
DESCRIPTION:Title: Symbolic regression and equation learning for identifying sparse non‑linear models\nLecturer: Georg Martius\n\nAbstract:\nIn classical machine learning, regression is treated as a black box process of identifying a suitable function without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. Following the theme of the lecture by Nathan Kutz, we will consider the search for parsimonious models. In this lecture we will consider non-linear models represented by concise analytical expressions. The problem of finding such expressions is generally called symbolic regression. Traditionally, this problem is solved with evolutionary search. In recent years, machine learning methods have been proposed for this task. One of the first works in this direction was the &ldquo;equation learner&rdquo; (EQL), which I will cover in some detail. Very recently, a different approach was presented that uses pretraining and language models to predict likely equations (NeSymReS). I will also talk about this method and compare them. An interesting aspect of the search for the most compact description of data is its connection to causal models and the identification of the true underlying relationships. In general, prior knowledge is required to make this work, but if successful, we can enjoy great generalization capabilities.
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-09/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211124T161500
DTEND:20211124T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 10 (TBA)
DESCRIPTION:Title: TBA\nLecturer: Dominik Janzing\n\nAbstract:\nTBA
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-10/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211201T161500
DTEND:20211201T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 11 (TBA)
DESCRIPTION:Title: TBA\nLecturer: Manuel Gomez Rodriguez\n\nAbstract:\nTBA
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-11/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211208T161500
DTEND:20211208T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

BEGIN:VEVENT
SUMMARY:[Beyond i.i.d. learning] Lecture 12 (TBA)
DESCRIPTION:Title: TBA\nLecturer: Lester Mackey\n\nAbstract:\nTBA
SEQUENCE:0
STATUS:CONFIRMED
URL:https://beyond-iid-learning.xyz/lectures/lecture-12/
DTSTAMP:20211117T204804Z
LAST-MODIFIED:20211117T204804Z
DTSTART:20211215T161500
DTEND:20211215T180000
LOCATION:https://zoom.us/j/62143808058
END:VEVENT

END:VCALENDAR